import urllib.parse
import requests  # For external API calls (replace with actual API keys)
from bs4 import BeautifulSoup  # For basic HTML parsing (optional)

# Replace with your API keys
VIRUSTOTAL_API_KEY = "YOUR_VIRUSTOTAL_API_KEY"  # Optional, for domain reputation

# Suspicious subdomain patterns (add/remove as needed)
SUSPICIOUS_SUBDOMAINS = [
    "gmail", "hotmail", "yahoo", "aol",  # Free email providers
    "login", "signin", "account",  # Common phishing subdomains
    "[0-9a-zA-Z]{10,}"  # Long, random alphanumeric sequences
]

# Cloud service domains commonly used for phishing
CLOUD_PHISHING_SERVICES = [
    "trycloudflare.com", "freenom.com", "vertex.ai",  # Examples
    "*.click", "*.tech", "*.xyz"  # Wildcards for common TLDs
]


def is_legitimate(url):
  """
  Analyzes a URL and classifies it as legitimate or phishing based on various checks.

  Args:
      url (str): The URL to be analyzed.

  Returns:
      tuple: A tuple containing the classification (str) and explanation (str).
  """

  explanation = ""

  # Basic URL analysis
  parsed_url = urllib.parse.urlparse(url)
  if len(url) > 75:
    explanation += " - URL length is excessively long.\n"

  # Special characters (consider adjusting based on your needs)
  special_chars = "~#?+=&*%"
  if any(char in url for char in special_chars):
    explanation += " - URL contains uncommon special characters.\n"

  # Protocol (HTTPS preferred)
  if parsed_url.scheme != "https":
    explanation += " - URL uses HTTP instead of HTTPS.\n"

  # Subdomain analysis
  subdomains = parsed_url.netloc.split(".")[:-1]
  for subdomain in subdomains:
    if any(pattern in subdomain for pattern in SUSPICIOUS_SUBDOMAINS):
      explanation += f" - Suspicious subdomain found: '{subdomain}'.\n"

  # Domain reputation (optional, requires API key)
  if VIRUSTOTAL_API_KEY:
    try:
      response = requests.get(f"https://www.virustotal.com/api/v3/domains/{parsed_url.netloc}", headers={"Authorization": f"Bearer {VIRUSTOTAL_API_KEY}"})
      data = response.json()
      if data.get("data", {}).get("attributes", {}).get("categories", [{}])[0].get("name") == "Suspicious":
        explanation += " - Domain has a suspicious reputation (VirusTotal).\n"
    except Exception:
      pass  # Handle potential API errors gracefully

  # Content analysis (optional, requires additional setup)
  # if content_analysis_enabled:
  #   try:
  #     response = requests.get(url)
  #     soup = BeautifulSoup(response.content, "html.parser")
  #     # Check for phishing indicators in the content (e.g., forms, messages)
  #   except Exception:
  #     pass  # Handle potential errors

  # Cloud service detection
  for domain in CLOUD_PHISHING_SERVICES:
    if parsed_url.netloc.endswith(domain) or parsed_url.netloc.startswith(f"{domain}."):
      explanation += f" - URL hosted on a cloud service commonly used for phishing: '{domain}'.\n"

  # Classification and explanation
  if explanation:
    return "Phishing", explanation.strip()
  else:
    return "Legitimate", "No suspicious indicators found."


def main():
  """
  Prompts the user for a URL and prints the classification with explanation.
  """
  url = input("Enter a URL: ")
  classification, explanation = is_legitimate(url)
  print(f"\nClassification: {classification}")
  print(f"Explanation:\n{explanation}")


if __name__ == "__main__":
  main()
